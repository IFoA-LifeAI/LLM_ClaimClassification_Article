{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de39f740",
   "metadata": {},
   "source": [
    "# Classifying Actuarial Data with LLMs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs) are increasingly playing a role in all types of office-based work, with actuarial being no exception. According to one recent study by the SOA [https://www.soa.org/4ac5cf/globalassets/assets/files/resources/research-report/2025/2025-11-ait-172-ai-quarter-survey-report.pdf] actuaries are currently most likely to use LLMs in the following ways:\n",
    "- Brainstorming\n",
    "- Writing and Interpreting Documents\n",
    "- Chatbots\n",
    "- Code Generation \n",
    "\n",
    "In this article, we consider a use case for an actuary to save on one of the more time-consuming tasks an actuary might experience. **Data Cleansing**. \n",
    "\n",
    "Normally data cleansing is carried out by code, data manipulation or in some cases manually. But occassionally our data is just too messy, nuanced and large for these options. It is possible to build a pipeline within Python, where new data is cleansed by an LLM. We present the results of this solution on a case study below. We also explore some more advanced techniques for getting the most out of our LLM.\n",
    "\n",
    "## Initial Case Study\n",
    "\n",
    "For this case study we have taken the ICD-10 causes of death as our dataset. There are thousands of causes of death in here, but we want to classify them into a small group of broader categories. The broader categories we have pulled from a paper online related to the cause-specific mortality impact of smoking [https://pmc.ncbi.nlm.nih.gov/articles/PMC3229033/]. To make our ICD-10 causes of death compatible with the hazard ratios in this paper, we wish to classify each of them into one of:\n",
    "\n",
    "    \"ischaemic heart disease\"\n",
    "    \"cerebrovascular disease\"\n",
    "    \"pulmonary disease\"\n",
    "    \"lung cancer\"\n",
    "    \"colorectal cancer\"\n",
    "    \"larynx cancer\"\n",
    "    \"kidney cancer\"\n",
    "    \"acute myeloid leukemia\"\n",
    "    \"oral cavity cancer\"\n",
    "    \"esophageal cancer\"\n",
    "    \"pancreatic cancer\"\n",
    "    \"bladder cancer\"\n",
    "    \"stomach cancer\"\n",
    "    \"prostate cancer\"\n",
    "    \"none\"\n",
    "\n",
    "To get a measure of the results, we also attempted this classification piece ourselves. We should caveate that our estimates are our own best guesses, and may be incorrect in places. After all, to err is human!\n",
    "\n",
    "We got some initial results through from the OpenAI gpt-o4 model. In general performance was quite strong, with an accuracy of [INSERT ACCURACY]. The performance is summarised below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table with TRUE/FALSE POSITIVE/NEGATIVE counts, as well as precision, recall and F1 scores \n",
    "# (i.e. table 3 in https://paulbeardactuarial.github.io/artificial_actuary_cod.html, but with gpt-4o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189571a9",
   "metadata": {},
   "source": [
    "## Logprobs \n",
    "\n",
    "*\"Doubt is an uncomfortable condition, but certainty is a ridiculous one\"* \n",
    "- Voltaire\n",
    "\n",
    "In our case study we completed the task manually so we could compare results. This is useful for scoring, but would defeat the purpose for practically use. Most people are familiar with the term \"hallucination\" in the LLM context, and we know that the LLM did not score 100%. Do we have to decide between double checking everything the LLM produces,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82961945",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_import_testdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_cod_data\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m cod_data, cod_vector = \u001b[43mload_cod_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_filepath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./Data/34506561512084822.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m results_df = pd.DataFrame({\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcause_of_death\u001b[39m\u001b[33m\"\u001b[39m: pd.Series(cod_vector, dtype=\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     14\u001b[39m })\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Lowercase cause_of_death to match everything\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulb\\Python\\LLM_ClaimClassification\\data_import_testdata.py:43\u001b[39m, in \u001b[36mload_cod_data\u001b[39m\u001b[34m(data_filepath, randomise)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03mLoad the cause-of-death data and return:\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m  cod_data: DataFrame with clean cause_of_death\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m  cod_vector: shuffled list of unique cause_of_death strings\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# read csv like readr::read_csv(..., skip = 10)\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# data = (\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m#     pd.read_csv(data_filepath, skiprows=10)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m#     .iloc[1:8225]  # R: data[2:8225,]\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m#     .copy()\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m cod_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m cod_data.columns = _clean_names(cod_data.columns)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# sum across all numeric columns except 'cause_of_death'\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# cols_to_sum = [c for c in data.columns if c != \"cause_of_death\"]\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# data[cols_to_sum] = data[cols_to_sum].apply(pd.to_numeric, errors=\"coerce\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[38;5;66;03m#     .astype(float)\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot of logprobs distributions of \"correct\" and \"incorrect\" responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413c14d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
